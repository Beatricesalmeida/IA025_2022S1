{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718 (175857)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/Pedro_Luis_Azevedo_Costa/Exerc%C3%ADcios_20210718_(175857).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe"
      },
      "source": [
        "print('Meu nome é: Pedro Luís Azevedo Costa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfd17c9-50c7-4c7f-d764-38dd394cd3f7"
      },
      "source": [
        "import collections \n",
        "def top_k(L, k):\n",
        "  counter = collections.Counter(L)    \n",
        "  lista_dict = list(counter.items())\n",
        "  lista_dict = sorted(lista_dict, key=lambda x: x[1], reverse=True)\n",
        "  lista_dict = lista_dict[0:k]\n",
        "  dict_reduced = dict(lista_dict)\n",
        "  return dict_reduced\n",
        "  \n",
        "top_k(L, 2)\n",
        "    # Escreva aqui o código"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 4, 'e': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45fb7f0-82c6-4721-b419-b3d8ab686cde"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4323968-2b62-490a-9b86-ddc6f25d9995"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 570 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738ed22a-2143-47a2-d897-09133980fef5"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "import re\n",
        "import collections\n",
        "\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "    # escreva o código aqui.\n",
        "\n",
        "    # Usa regex pare segmentear a string em uma lista de palavras\n",
        "    text = text.lower()\n",
        "    word_list = re.findall(r'\\w+|[?.\\-\",]+', text)\n",
        "    #Cria uma lista de '-1'\n",
        "    converted_list = len(list(word_list)) * [-1]\n",
        "    keys = vocabulary.keys()\n",
        "    # Itera sobre os valores para achar quais fazem parte do dicionario\n",
        "    for i in range(0,len(word_list)):\n",
        "      if word_list[i] in keys:\n",
        "        converted_list[i] = vocabulary[word_list[i]]\n",
        "\n",
        "    return(converted_list)\n",
        "\n",
        "\n",
        "tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 2, 4, -1, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07de8b07-f476-4be8-a59f-6950804f4e77"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8565c6c-14be-43dc-987f-dff46ea969d7"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 4.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100000 loops, best of 5: 4.37 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "#Baseado na solução do Leonardo Augusto da Silva Pacheco. RA 201914, que sua vez foi inspirada no algoritmo Reservoir Sampling\n",
        "def sample(path: str, k: int):\n",
        "  file = open(path)\n",
        "  list_line = []\n",
        "  for pos, line in enumerate(file):\n",
        "    if pos < k:\n",
        "      list_line.append(line)\n",
        "    else:\n",
        "      aleat = randint(0, pos)\n",
        "      #Sempre que o valor aleatorio for menor que o numero de amostras, a linha é copiada para a posicao aleat na string\n",
        "      if aleat < k:\n",
        "        list_line[aleat] = line\n",
        "  return list_line\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db5a4ea-d067-476d-ac36-2f5c5f01bfc4"
      },
      "source": [
        "\n",
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 81\\n', 'line 69\\n', 'line 91\\n', 'line 20\\n', 'line 19\\n', 'line 56\\n', 'line 45\\n', 'line 52\\n', 'line 39\\n', 'line 79\\n']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38466677-4614-474b-fdc9-eccbaeedea2f"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.55 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: $p × m × (n - 1)$ \n",
        "- número de multiplicações: $m × n × p$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88c1ab2-a544-499b-d433-cb02eb2eb6cb"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88356a2f-2f2c-4aa1-fabc-9a3ccfae89ed"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "def get_row_avg(A_np):\n",
        "  avg_np = np.mean(A_np, axis=1)\n",
        "  print(avg_np)\n",
        "  return avg_np\n",
        "\n",
        "get_row_avg(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.5  8.5 14.5 20.5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688e0f83-7f39-49ae-b8c0-3e2139eac2ff"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "def normalize(A_np):\n",
        "  \"\"\"\n",
        "  Entrada: Matriz numpy\n",
        "  Saída: Matriz numpy\n",
        "  \"\"\"\n",
        "  # Acha valores máximos e mínimos\n",
        "  max_A = A_np.max()\n",
        "  min_A = A_np.min()\n",
        "\n",
        "  # Calculo de normalização\n",
        "  norm_A = (A_np - min_A)/(max_A - min_A)\n",
        "  return norm_A\n",
        "\n",
        "normalize(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.04347826, 0.08695652, 0.13043478, 0.17391304,\n",
              "        0.2173913 ],\n",
              "       [0.26086957, 0.30434783, 0.34782609, 0.39130435, 0.43478261,\n",
              "        0.47826087],\n",
              "       [0.52173913, 0.56521739, 0.60869565, 0.65217391, 0.69565217,\n",
              "        0.73913043],\n",
              "       [0.7826087 , 0.82608696, 0.86956522, 0.91304348, 0.95652174,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad471e0-9015-437b-efde-2d85abb205f3"
      },
      "source": [
        "def normalize_column(A_np):\n",
        "  \"\"\"\n",
        "  Entrada: Matriz numpy\n",
        "  Saída: Matriz numpy\n",
        "  \"\"\"\n",
        "  # Acha valores máximos e mínimos das colunas (axis=0)\n",
        "  max_A = A_np.max(axis=0)\n",
        "  min_A = A_np.min(axis=0)\n",
        "\n",
        "  # Calculo de normalização\n",
        "\n",
        "  A_np = (A_np - min_A)/(max_A - min_A)\n",
        "\n",
        "  return A_np\n",
        "\n",
        "normalize_column(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
              "        0.33333333],\n",
              "       [0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n",
              "        0.66666667],\n",
              "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16ac5e1-e55b-44e7-96c6-a66932bdb8bd"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "def normalize_row(A_np):\n",
        "  \"\"\"\n",
        "  Entrada: Matriz numpy\n",
        "  Saída: Matriz numpy\n",
        "  \"\"\"\n",
        "  # Acha valores máximos e mínimos das linhas (axis=1)\n",
        "  max_A = A_np.max(axis=1)\n",
        "  min_A = A_np.min(axis=1)\n",
        "\n",
        "  # Faz a transposição da matriz para permitir a operação a nível de linhas\n",
        "  A_np = A_np.T\n",
        "  # Calculo de normalização\n",
        "  A_np = (A_np - min_A)/(max_A - min_A)\n",
        "  # Retorna a matriz à forma original\n",
        "  A_np = A_np.T\n",
        "\n",
        "  # Calculo de normalização\n",
        "  return A_np\n",
        "\n",
        "normalize_row(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f209e3-43a0-4b94-e029-e2adec83813b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "\n",
        "    # Escreva sua solução aqui.\n",
        "    \n",
        "    # Aproveita a solução de normalização por linhas do item anterior\n",
        "    A = normalize_row(A)\n",
        "\n",
        "    #Acha o valor de soma de cada linha\n",
        "    soma_linha = A.sum(axis=1)\n",
        "    #Divide cada valor pela soma de sua linha. As operações de transposição foram feitas de modo a adequar o formato da matriz.\n",
        "    soft_A = (A.T/soma_linha).T\n",
        "    return soft_A\n",
        "\n",
        "softmax(A)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n",
              "        0.33333333],\n",
              "       [0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n",
              "        0.33333333],\n",
              "       [0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n",
              "        0.33333333],\n",
              "       [0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n",
              "        0.33333333]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3344b29c-5dfb-4785-a2bc-df8bfe4c7998"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00149626, 0.        , 0.99850374],\n",
              "       [0.        , 0.44444444, 0.55555556]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f80c4c6-d797-4b07-b836-8a9f6e14ae0d"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e12478a-4b7d-4afa-e12b-e2df4ed160db"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.96 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 130 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8828da3b-0cb3-461c-a731-124d47c94da8"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "    # Escreva seu código aqui.\n",
        "\n",
        "    # Cria um vetor numpy de duas dimensoes nxm, sendo 'n' o tamanho de y e 'm' o numero de classes \n",
        "    one_hot = np.zeros((y.size, n_classes))\n",
        "    # Cria um vetor numpy com o tamanho de y, e seta todos os valores da posição 'y' como 1. np.arange(y.size) define a coordenada 'n' e y, a coordenada 'm'\n",
        "    one_hot[np.arange(y.size),y] = 1\n",
        "    return one_hot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960649bc-75c6-47ad-9c7d-60484ad4ee39"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 194 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "class Normalizer:\n",
        "  def __init__(self, array_param):\n",
        "    \"\"\"\n",
        "    Entrada: Vetor numpy\n",
        "    Saída: Null\n",
        "\n",
        "    Recebe o vetor que terá os dados de média e desvio padrão replicados\n",
        "    \"\"\"\n",
        "    array_param = np.array(array_param)\n",
        "    self.avg = array_param.mean()\n",
        "    self.desv = array_param.std()\n",
        "\n",
        "  def __call__(self, array_to_be_normalized):\n",
        "    \"\"\"\n",
        "    Entrada: Vetor numpy\n",
        "    Saída: Vetor numpy\n",
        "\n",
        "    Retorna o vetor normalizado com a média e desvio padrão desejado\n",
        "    \"\"\"\n",
        "    array_to_be_normalized = np.array(array_to_be_normalized)\n",
        "    norm_avg = array_to_be_normalized.mean()\n",
        "    norm_desv = array_to_be_normalized.std()\n",
        "    # No calculo abaixo fazemos a operação de 'standardization' do vetor para deixar a média = 0 e desv_pad = 1. Logo em seguida somamos a media desejada e multiplicamos pelo desvio padrao do vetor que inicializou a classe\n",
        "    array_to_be_normalized = (array_to_be_normalized - norm_avg)/norm_desv *self.desv + self.avg\n",
        "    return array_to_be_normalized\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3ee441-b6a6-4581-d947-af7e7f9dac05"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "460f38cb-7b53-4ef7-f4d6-0f6eea319361"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ee4074-9b6f-466e-895d-411d994388aa"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab8c337-1d9b-4444-da3a-2589182fb808"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae2d16f-da83-4469-93f9-f73d3511c8f9"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4976332-81b6-4752-9a17-1e0b71bf1c78"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafd4cdd-96cb-4a38-845a-4c4d55850f7c"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df4ad23-5fe2-4b98-8514-f54ddaef7d5d"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3609894b-d3f8-4d97-a1c0-cb70944e6da3"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    J = ((x*w-y)**2).sum()\n",
        "    return J\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "delta = 0.00001\n",
        "grad = (J_func(w+delta, x,y) - J_func(w-delta,x,y))/(2*delta)\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0380)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54c565d0-eaf7-4922-b518-05113ee8e30f"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "J_list = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    J_list.append(J)\n",
        "    print('J=', J)\n",
        "    grad = (J_func(w+delta, x,y) - J_func(w-delta,x,y))/(2*delta)\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate *grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(J_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0380)\n",
            "w = tensor([1.2804])\n",
            "i = 1\n",
            "J= tensor(7.2499)\n",
            "grad = tensor(-20.1702)\n",
            "w = tensor([1.4821])\n",
            "i = 2\n",
            "J= tensor(3.7553)\n",
            "grad = tensor(-14.5197)\n",
            "w = tensor([1.6273])\n",
            "i = 3\n",
            "J= tensor(1.9449)\n",
            "grad = tensor(-10.4487)\n",
            "w = tensor([1.7318])\n",
            "i = 4\n",
            "J= tensor(1.0073)\n",
            "grad = tensor(-7.5221)\n",
            "w = tensor([1.8070])\n",
            "i = 5\n",
            "J= tensor(0.5216)\n",
            "grad = tensor(-5.4121)\n",
            "w = tensor([1.8611])\n",
            "i = 6\n",
            "J= tensor(0.2701)\n",
            "grad = tensor(-3.8937)\n",
            "w = tensor([1.9000])\n",
            "i = 7\n",
            "J= tensor(0.1399)\n",
            "grad = tensor(-2.8022)\n",
            "w = tensor([1.9281])\n",
            "i = 8\n",
            "J= tensor(0.0724)\n",
            "grad = tensor(-2.0169)\n",
            "w = tensor([1.9482])\n",
            "i = 9\n",
            "J= tensor(0.0375)\n",
            "grad = tensor(-1.4514)\n",
            "w = tensor([1.9627])\n",
            "i = 10\n",
            "J= tensor(0.0194)\n",
            "grad = tensor(-1.0444)\n",
            "w = tensor([1.9732])\n",
            "i = 11\n",
            "J= tensor(0.0101)\n",
            "grad = tensor(-0.7516)\n",
            "w = tensor([1.9807])\n",
            "i = 12\n",
            "J= tensor(0.0052)\n",
            "grad = tensor(-0.5409)\n",
            "w = tensor([1.9861])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3892)\n",
            "w = tensor([1.9900])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2801)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2016)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1451)\n",
            "w = tensor([1.9963])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1044)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0751)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.2033e-05)\n",
            "grad = tensor(-0.0541)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14eec30e-d12c-48a6-cd4d-1f577eba2dc7"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "J_list = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    J_list.append(J.detach().numpy())\n",
        "    print('J=', J)\n",
        "    w.retain_grad()\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    w =  w - learning_rate *grad\n",
        "    print('w =', w)\n",
        "# Plote aqui a loss pela iteração\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(J_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: \n",
        "\n",
        "o valor de $\\Delta w$ precisa satisfazer duas condições:\n",
        "- **Não ser de uma grandeza muito elevada** -> Valores de $\\Delta w$ muito grandes geram passos grandes no gradiente, o que implica em uma resposta de baixa precisão.\n",
        "- **Não pode ser muito pequeno** -> Um $\\Delta w$ muito pequeno pode implicar na não convergência do resultado para o número de iterações utilizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) $O(N^2)$ pois neste método precisamos fazer $N$ cálculos iterados sobre o vetor de gradiente, cujo tamanho é $N$. \n",
        "\n",
        "b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}