{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/Gleyson_Nascimento/resolu%C3%A7%C3%A3o_ex01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch para seleção de estudantes especiais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436dea41-24c2-452f-ca75-ca32b3f32813"
      },
      "source": [
        "print('Meu nome é: Gleyson Roberto do Nascimento - RA: 043801')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Gleyson Roberto do Nascimento - RA: 043801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "from collections import Counter\n",
        "\n",
        "def top_k(L, k):\n",
        "#Inserindo variáveis locais: \n",
        "    contador = Counter(L) #aplicando a função Counter na lista de entrada\n",
        "    contador = contador.most_common(k) #encontrando os top k da lista de entrada \n",
        "    dic = dict(contador)# colocando os top k no padrão de dicionário\n",
        "    return dic #retornando o dicionário"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de8f3ef-44e5-4a81-fb97-2c44fdd8948d"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0054dd1d-e2d4-4806-b613-8929d9af234f"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 585 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import re\n",
        "\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "  textl = text.lower() #padronizando a entrada para caixa baixa\n",
        "  tokens = re.findall('\\w+|[?.\\-\",]+',textl) #tokenizando por regex\n",
        "  ids = [vocabulary[word] if word in vocabulary.keys() else vocabulary['unknown']\n",
        "         for word in tokens] #fazendo busca dos ids correspondentes aos tokens no vocabulario\n",
        "  return ids # retornando a lista de identificadores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b59d7ab-7a21-46f8-b0bc-ef0937311b9b"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07294f21-4494-479f-ac36-4b1dca6da6e5"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.71 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "#Importando as bibliotecas necessárias para o exercício:\n",
        "from random import randint\n",
        "\n",
        "def sample(path: str, k: int):\n",
        "  \n",
        "  samples = [None]*k #criando uma lista para receber os samples\n",
        "  \n",
        "  #utilizando o algoritmo simples de reserva de sample\n",
        "  for index, line in enumerate(open(path, 'r')):\n",
        "    if index < k:\n",
        "      samples[index]=line[:-1]\n",
        "    else:\n",
        "      i = randint(0,index)\n",
        "      if i < k:\n",
        "        samples[i] = line[:-1]\n",
        "\n",
        "  return samples #retornando os itens amostrados "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1466b829-3529-4e09-b066-e95855d07c9c"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 19', 'line 64', 'line 58', 'line 65', 'line 94', 'line 36', 'line 48', 'line 25', 'line 42', 'line 69']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9f2b8a-f16d-4b05-fb10-93906ce9a1f8"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.53 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantas operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas:   $  mnp - mp = mp(n-1)$ flops\n",
        "- número de multiplicações: $  mnp$ flops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93515fdb-da0e-4bee-8ad2-ecb157128de0"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9aa9da-344c-49f8-d787-a0afa4cebb30"
      },
      "source": [
        "#Solução aplicada: utilizar a função np.mean diretamente\n",
        "medias_linhas = np.mean(A,axis=1) #com axis = 1 será calculada a média para cada linha \n",
        "print(medias_linhas) #imprimindo o array com a média de cada linha "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.5  8.5 14.5 20.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import numpy as np\n",
        "\n",
        "def normalizarm(A):\n",
        "  Amin = np.min(A) #encontrando o valor mínimo da matriz de entrada pela função np.min\n",
        "  Amax = np.max(A) #encontrando o valor máximo da matriz de entrada pela função np.max\n",
        "  delta = Amax-Amin #criando uma variável delta\n",
        "  C = (A/delta)-(Amin/delta) #fazendo o cálculo da matriz normalizada com base na matriz de entrada, delta e valor mínimo\n",
        "  return C #retornando a matriz normalizada  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtLHh875uoUh",
        "outputId": "d881bb49-93ec-4246-b73c-cca9793e9d5f"
      },
      "source": [
        "#Testando a aplicação:\n",
        "C=normalizarm(A)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import numpy as np\n",
        "\n",
        "def normalizarmc(A):\n",
        "  Amin = np.min(A, axis=0) #descobrindo o valor mínimo por coluna através da função np.min com axis = 0\n",
        "  Amax = np.max(A, axis=0) #descobrindo o valor máximo por coluna através da função np.max com axis = 0\n",
        "  delta = Amax-Amin #criando uma variável delta\n",
        "  Ccol = (A/delta)-(Amin/delta) #fazendo a normalização por coluna considerando todas as linhas, delta e valor mínimo \n",
        "  return Ccol #retornando a matriz normalizada por coluna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKzgn9txwdGg",
        "outputId": "16e07ade-076d-4c85-f1d3-04028d0bc734"
      },
      "source": [
        "#Testando a aplicação:\n",
        "Ccol=normalizarmc(A)\n",
        "print(Ccol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import numpy as np\n",
        "\n",
        "def normalizarmr(A):\n",
        "  Amin = np.min(A, axis=1, keepdims=True) #descobrindo o valor mínimo por linha através da função np.min com axis = 1 e mantendo as dimenões\n",
        "  Amax = np.max(A, axis=1,  keepdims=True) #descobrindo o valor máximo por linha através da função np.max com axis = 1 e mantendo as dimensões\n",
        "  delta = Amax-Amin #criando uma variável delta\n",
        "  Crow = (A/delta)-(Amin/delta) #fazendo a normalização por linha considerando todas as colunas, delta e valor mínimo\n",
        "  return Crow #retornando a matriz normalizada por linha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9UYkWjqPHh4",
        "outputId": "8bb12b4c-a039-42d2-b25e-e741a4df81c8"
      },
      "source": [
        "#Testando a aplicação:\n",
        "Crow=normalizarmr(A)\n",
        "print(Crow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs1Lh6BcBT0r"
      },
      "source": [
        "De forma geral, a função softmax pode ser descrita como:\n",
        "\n",
        "$\\sigma(A)=\\frac{e^{A_{i}}}{\\sum_{i=1}^k {e^{A_{i}}}}=\\frac{\\frac{e^{A_{i}}}{e^m}}{\\sum_{i=1}^k \\frac{e^{A_{i}}}{e^m}}=\\frac{e^{A_{i}-m}}{\\sum_{i=1}^k {e^{A_{i}-m}}}$ , com $m=max(A)$\n",
        "\n",
        "De forma que esta representação consegue evitar o overflow computacional devido a presença de exponenciais na equação.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    expoA = np.exp(A - np.max(A, axis =1,keepdims=True)) #fazendo a exponencial da diferença matriz A e seu máximo no eixo de linha e mantendo as dimensões para estabilidade matricial\n",
        "    softmax = (expoA / expoA.sum(axis=1,keepdims=True)) #fazendo a divisão entre expoA e o somatório de expoA no eixo de linha e mantendo as dimensões para estabilidade matricial\n",
        "    return softmax #retornando a função softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c702680e-0652-442d-b808-5d45a5e10bf1"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c410c5b-65ac-49c0-edaf-9478f9b1155d"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42f5f76-440f-4e9f-9298-05ac53f241f6"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 292 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400f19d0-bf37-4976-b223-9a0af92c7d3a"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import numpy as np\n",
        "\n",
        "def one_hot(y, n_classes):\n",
        "    shape = (y.size, n_classes) #definindo o shape da matriz one_hot com base no tamanho da matriz de entrada e do número de classes\n",
        "    one_hot = np.zeros(shape) #criando uma matriz de zeros com o shape definido anteriormente\n",
        "    rows = np.arange(y.size) #criando uma lista com valores de 0 até o tamanho da matriz de entrada\n",
        "    one_hot[rows, y] = 1 #convertendo a matriz de entrada conforme o número de classes em one hot\n",
        "    return one_hot #retornando a matriz one hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe3574d-3ae4-4364-cee6-42e547cd6960"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 4 8 6 8 8 5 4 2 6]\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0b872c-d64e-4b45-9cfe-5d27c5e33e97"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48d8082-7a50-4209-b7d0-f2997658d38e"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 187 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXvgkoaCuaDN"
      },
      "source": [
        "#Criando a classe Normalizer\n",
        "class Normalizer:\n",
        "\n",
        "  def __init__(self, va: np.ndarray): #ativando a função de construção\n",
        "    self.va = va #guardando a matriz de entrada inicial\n",
        "\n",
        "  def __call__(self, vb: np.ndarray): #ativando a função de call\n",
        "    self.vb=vb #guardando a segunda matriz de entrada\n",
        "    self.normalizacaob = (self.vb - np.mean(self.vb))/np.std(self.vb) #normalizando a segunda matriz de entrada \n",
        "    self.normalizacao = np.mean(self.va) + self.normalizacaob*np.std(self.va) #levando a segunda matriz normalizada para a mesma média e desvio-padrão da matriz inicial\n",
        "    return self.normalizacao #retornando a matriz normalizada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87e8d54-58f7-42fa-bf29-3b12ce7ce40e"
      },
      "source": [
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b) #atribuindo a classe Normalizer para normalize\n",
        "normalized_array = normalize(array_a) #fazendo call dentro da classe normalize\n",
        "print(normalized_array) #imprimindo o array normalizado"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9299783f-beff-44bf-a207-c011f2cb3342"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8e19b4-670e-4835-f551-1e6595a4aead"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9dc027-575b-4fc1-9542-950d5abe3def"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c6cab-b823-484f-ae83-03c90dcd1c5e"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzmDPAzIV04p",
        "outputId": "02249ba5-a38f-499f-95d0-640f3bd8a9d0"
      },
      "source": [
        "type(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbd95d2-8150-41bd-ee32-3c3567f84b9f"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294e278a-bdef-4cfa-83f7-6d8bfb715a04"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a03d822-34a0-4832-c015-3601eba0504b"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd04fe69-5b7d-4218-d718-de72718b9f35"
      },
      "source": [
        "#Importando a biblioteca necessária para o exercício:\n",
        "import torch\n",
        "\n",
        "#Criando a função de loss (J_func) com os parâmetros dados\n",
        "def J_func(w, x, y):\n",
        "  y_pred = x * w\n",
        "  e = y_pred - y\n",
        "  e2 = e.pow(2)\n",
        "  J = e2.sum()\n",
        "  return J\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "delta_w = 1 #adotando um delta_w pequeno\n",
        "grad = (J_func(w+delta_w,x,y)-J_func(w-delta_w,x,y))/2*delta_w #aplicando a equação do gradiente por diferenças finitas\n",
        "print('grad=', grad) #imprimindo o valor do gradiente e conferindo com o método anterior"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRZOV9qYrwu"
      },
      "source": [
        "Desta forma, os gradientes em -28 batem e as metodologias de backward e diferenças finitas leavm ao mesmo resultado, mas com custos computacionais distintos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df94907a-014e-454a-e261-0f5308e00e43"
      },
      "source": [
        "#Importando as bibliotecas necessárias para o exercício:\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "loss = [] #criando uma lista vazia para receber os valores de loss\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    delta_w = 1 #adotando um delta_w pequeno\n",
        "    grad = (J_func(w+delta_w,x,y)-J_func(w-delta_w,x,y))/2*delta_w #aplicando a equação do gradiente por diferenças finitas\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate*grad #aplicando a fórmula de minimização\n",
        "    print('w =', w)\n",
        "    print('\\n') #imprimindo um \\n para facilitar a visualização\n",
        "    loss.append(J) #fazendo append dos valores de J na lista loss\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.title('Gráfico da loss J pela iteração i') #título do gráfico\n",
        "plt.xlabel('Iteração (i)') #título do eixo x do gráfico\n",
        "plt.ylabel('Loss (J)') #título do eixo y do gráfico\n",
        "plt.plot(loss) #fazendo o plot do loss\n",
        "plt.show() #show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.)\n",
            "w = tensor([1.2800])\n",
            "\n",
            "\n",
            "i = 1\n",
            "J= tensor(7.2576)\n",
            "grad = tensor(-20.1600)\n",
            "w = tensor([1.4816])\n",
            "\n",
            "\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor(-14.5152)\n",
            "w = tensor([1.6268])\n",
            "\n",
            "\n",
            "i = 3\n",
            "J= tensor(1.9504)\n",
            "grad = tensor(-10.4509)\n",
            "w = tensor([1.7313])\n",
            "\n",
            "\n",
            "i = 4\n",
            "J= tensor(1.0111)\n",
            "grad = tensor(-7.5247)\n",
            "w = tensor([1.8065])\n",
            "\n",
            "\n",
            "i = 5\n",
            "J= tensor(0.5241)\n",
            "grad = tensor(-5.4178)\n",
            "w = tensor([1.8607])\n",
            "\n",
            "\n",
            "i = 6\n",
            "J= tensor(0.2717)\n",
            "grad = tensor(-3.9008)\n",
            "w = tensor([1.8997])\n",
            "\n",
            "\n",
            "i = 7\n",
            "J= tensor(0.1409)\n",
            "grad = tensor(-2.8086)\n",
            "w = tensor([1.9278])\n",
            "\n",
            "\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0222)\n",
            "w = tensor([1.9480])\n",
            "\n",
            "\n",
            "i = 9\n",
            "J= tensor(0.0379)\n",
            "grad = tensor(-1.4560)\n",
            "w = tensor([1.9626])\n",
            "\n",
            "\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0483)\n",
            "w = tensor([1.9730])\n",
            "\n",
            "\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7548)\n",
            "w = tensor([1.9806])\n",
            "\n",
            "\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5434)\n",
            "w = tensor([1.9860])\n",
            "\n",
            "\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3913)\n",
            "w = tensor([1.9899])\n",
            "\n",
            "\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2817)\n",
            "w = tensor([1.9928])\n",
            "\n",
            "\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "\n",
            "\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "\n",
            "\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1051)\n",
            "w = tensor([1.9973])\n",
            "\n",
            "\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "\n",
            "\n",
            "i = 19\n",
            "J= tensor(5.3059e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "806686ed-c266-40ac-e894-80b8b7f182f6"
      },
      "source": [
        "#Importando as bibliotecas necessárias para o exercício:\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Criando a função gradi com a definição anterior de gradiente por backward\n",
        "def gradi(w1,J1):\n",
        "  if w1.grad: w1.grad.zero_()\n",
        "  J1.backward()\n",
        "  return w1.grad\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "loss1 = [] #criando uma lista vazia para receber os valores de loss\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    grad = gradi(w,J) #aplicando a função gradi para obter o gradiente\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate*grad #aplicando a fórmula de minimização\n",
        "    w.retain_grad() #fazendo a retenção de grafo\n",
        "    print('w =', w)\n",
        "    print('\\n') #imprimindo um \\n para facilitar a visualização\n",
        "    loss1.append(J.detach().numpy()) #fazendo append dos valores de J na lista loss\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "plt.title('Gráfico da loss J pela iteração i') #título do gráfico\n",
        "plt.xlabel('Iteração (i)') #título do eixo x do gráfico\n",
        "plt.ylabel('Loss (J)') #título do eixo y do gráfico\n",
        "plt.plot(loss1) #fazendo o plot do loss\n",
        "plt.show() #show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?\n",
        "\n",
        "Resposta: Para este caso, $\\Delta w$ possui duas restrições: a) **valores muito pequenos**, uma vez que passos muito pequenos levam a erros de precisão computacional, b) **valores muito grandes**, uma vez que estes trazem a não linearidade para a diferenciação e, portanto, não geram a diferenciação necessária para que o método seja eficiente. Para o exercício proposto, 1 foi a melhor solução."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) Para o método das diferenças finitas, supondo que para executar J haverá um custo computacional $O(N)$ para cada termo no somatório, então, como temos 2N cálculos de J podemos dizer que o custo computacional será $O(N^2)$ \n",
        "\n",
        "b) Para o método de backpropagation, como ocorre apenas uma operação para o cálculo de gradiente, passando pelos valores na ida e atualizando na volta podemos dizer que o custo será de $O(2N)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:\n",
        "\n",
        "No começo do treinamento podemos supor que as probabilidades para cada classe serão idênticas, de forma que $p_j=\\frac{1}{K}$, assim, como $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário, logo, para um classificador inicializado aleatoriamente, teremos:\n",
        "\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j = -1 \\log \\frac{1}{K} = \\log{K}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}